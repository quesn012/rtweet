C <- C[D]
P <- P[D]
N <- length(P)
if (B < min(C) | N <= 0L | B <= 0L) return(0L)  #problem is NA's are in the list of P here
if (N == 1L) return(P[1])
if (N == 2L) return(max(P))
max(
( ifelse(B - C[1]>=0, (q3(B - C[1], C[3:N], P[3:N]) + P[1]), 0)),
( q3(B, C[2:N], P[2:N]))
)
})
# Please print the result of that line below
q3(B = 11L, C = c(2L, 1L, 4L, 3L, 1L),P = c(9L, 4L, 17L, 8L, 3L))
#b = 4L
#c = c(2L, 1L, 4L, 3L, 1L)
#p = c(9L, 4L, 17L, 8L, 3L)
# Chunk 1
# set global chunk options
library(knitr)
opts_chunk$set(echo=TRUE)
suppressPackageStartupMessages({
library(TSA)
library(jsonlite) # in order to use fromJSON
library(memoise)
library(assertthat)
library(xts)
library(forecast)
library(ggplot2)
library(dplyr)
library(lubridate)
})
set.seed(123456)
# Chunk 2
btc_data <- fromJSON("btc.json") # please do not change this line!
btc_price <- flatten(data.frame(btc_data$bpi)) %>% t()
btc_dates <- row.names(btc_price)
btc_dates <- substring(btc_dates, 2)
btc_df <- data.frame(bpi = btc_price, date = btc_dates)
btc_df$date <- gsub("[.]", "/", btc_df$date)
btc_df$date <- ymd(btc_df$date)
btc_ts <- ts(btc_df$bpi)
# Chunk 3
#plot general timeline first
autoplot(btc_ts)
# Chunk 4
#Let's examine it by years.
#making a dataframe so we can look at each year on top of each other
btc_year <- data.frame(year1 = btc_ts[1:365], year2 = btc_ts[366:730], year3 = btc_ts[731:1095], year4 = btc_ts[1096:1460])
#year 1
autoplot(ts(btc_ts[1:365]))
#year 2
autoplot(ts(btc_ts[366:730]))
#year 3
autoplot(ts(btc_ts[731:1095]))
#year 4
autoplot(ts(btc_ts[1096:1462]))
# Chunk 5
#all 4 years together
ggplot(btc_year) + geom_line(aes(1:365,year1, color='year1')) + geom_line(aes(1:365,year2, color='year2')) + geom_line(aes(1:365,year3, color='year3')) + geom_line(aes(1:365,year4, color='year4'))
# Chunk 6
ggplot(btc_year) + geom_line(aes(1:365,year1, color='year1')) + geom_line(aes(1:365,year2, color='year2')) + geom_line(aes(1:365,year3, color='year3'))
# Chunk 7
btc_df$month <- month(btc_df$date)
btc_gb_month <- btc_df %>% group_by(month) %>%
summarise(mean_price = mean(bpi))
ggplot(btc_gb_month) + geom_line(aes(month,mean_price))
# Chunk 8
btc_df$month <- month(btc_df$date)
btc_gb_month2 <- btc_df[1:1096,] %>% group_by(month) %>%
summarise(mean_price = mean(bpi))
ggplot(btc_gb_month2) + geom_line(aes(month,mean_price))
# Chunk 9
#check within month seasonality - take 5 random months and plot them together
btc_month <- data.frame(month1 = btc_ts[1:30], month2 = btc_ts[241:270], month3 = btc_ts[601:630],
month4 = btc_ts[991:1020], month5 = btc_ts[721:750])
#taking months in the first 3 years since the 4th blows up the scale
ggplot(btc_month) + geom_line(aes(1:30,month1, color='month1')) +
geom_line(aes(1:30,month2, color='month2')) +
geom_line(aes(1:30,month3, color='month3')) +
geom_line(aes(1:30,month4, color='month4')) +
geom_line(aes(1:30,month5, color='month5'))
# Chunk 10
#check within month seasonality - take 5 random months and plot them together
btc_week <- data.frame(week1 = btc_ts[1:7], week2 = btc_ts[141:147], week3 = btc_ts[281:287],
week4 = btc_ts[421:427], week5 = btc_ts[561:567],week6 = btc_ts[701:707],
week7 = btc_ts[841:847],week8 = btc_ts[981:987])
#taking months in the first 3 years since the 4th blows up the scale
ggplot(btc_week) + geom_line(aes(1:7,week1, color='week1')) +
geom_line(aes(1:7,week2, color='week2')) +
geom_line(aes(1:7,week3, color='week3')) +
geom_line(aes(1:7,week4, color='week4')) +
geom_line(aes(1:7,week5, color='week5')) +
geom_line(aes(1:7,week6, color='week6')) +
geom_line(aes(1:7,week7, color='week7')) +
geom_line(aes(1:7,week7, color='week8'))
# Chunk 11
btc_df$week <- weekdays.Date(btc_df$date)
btc_gb_week <- btc_df %>% group_by(week) %>%
summarise(mean_price = mean(bpi))
ggplot(btc_gb_week) + geom_bar(stat='identity',aes(week,mean_price))
# Chunk 12
#stationary test
adf.test(btc_ts)
#can see its not stationary. definetly a random walk going on here.
adf.test(diff(btc_ts))
#with lag we do get stationary
# Chunk 13
#acf
ggAcf(btc_ts)
# Chunk 14
ggAcf(diff(btc_ts))
# Chunk 15
#pacf
ggPacf(btc_ts)
# Chunk 16
ggPacf(diff(btc_ts))
# Chunk 17
#eacf
eacf(diff(btc_ts))
# Chunk 18
#try auto arima
auto.arima(btc_ts, stationary = FALSE, seasonal = FALSE)
# Chunk 19
btc_train <- btc_ts[1:1096]
btc_test <- btc_ts[1096:1462]
# Chunk 20
naive_arima_forecast <- naive(btc_train, h=length(btc_test))
accuracy(naive_arima_forecast, btc_test)
# Chunk 21
arima_110 <- Arima(btc_train, order=c(1,1,0))
arima_110_forecast <- forecast(arima_110, h=length(btc_test))
accuracy(arima_110_forecast,btc_test)
# Chunk 22
arima_111 <- Arima(btc_train, order=c(1,1,1))
arima_111_forecast <- forecast(arima_111, h=length(btc_test))
accuracy(arima_110_forecast,btc_test)
# Chunk 23
arima_010 <- Arima(btc_train, order=c(1,1,1))
arima_010_forecast <- forecast(arima_111, h=length(btc_test))
accuracy(arima_010_forecast,btc_test)
# Chunk 24
final_arima <- Arima(btc_ts,order=c(0,1,0))
final_forecast <- forecast(final_arima,h=365)
# Chunk 25
# Produce a ggplot
final_arima_df <- data.frame(mean = final_forecast$mean, upper = final_forecast$upper, lower = final_forecast$lower, date = 1:365)
p1 <- ggplot(final_arima_df) + geom_line(aes(date,mean))
# Don't forget to plot it!
p1
# Please save your forecast for the next 365 days
future_btc <- rep(final_forecast$mean, 365)
# Provide a 95% prediction interval for Bitcoin price in a year from now (365 days)
low <- final_forecast$lower[365]
high <- final_forecast$upper[365]
# Please print them
low
high
# Chunk 26
# Must be a number 1, -1 or 0
answer1 <- 0 # 1 = will go up, -1 = will go down, 0 = can't tell
# Please print it
answer1
# Chunk 27
q2 <- memoise(function(P) {
if (!is.integer(P)) stop ("P must contain integers")
N <- length(P) # the total number of supercharger locations
# N should *NOT* be hard coded. The bot will try to feed P which has 10 locations or more.
# Your solution should work for any vector P of any length
# Write your code here
#base case
if (N <= 0L) return(0L)
if (N == 1L) return(P[1])
if (N == 2L) return(max(P))
max(
q2(P[3:N]) + P[1],
q2(P[2:N])
)
})
#q2(P = c(2L,6L,5L)) #7
#q2(P = c(1L,5L)) #5
#q2(P = c(6L,2L,3L,5L)) #11 - 6 + 5
#q2(P = c(7L,9L,4L,5L,12L,8L)) #should be 23 (12 + 4 + 7)
#q2(P = c(10L,1L,13L,12L,2L,17L,18L,10L,7L,30L,6L))
# Chunk 28
# Some manual cases and expected answers
# Basic "sanity" checks
# if only two locations are available you should pick the one with the highest profit
validate_that(q2(P = c(1L,5L)) == 5L)
# greedy solutions don't work:
# say, consider (2,6,5)
# while 6 is the most profitable location it is better to pick 2 + 5
# since they are non-adjacent and can give higher overall profit
# you cannot pick 6 + 5 since they are adjacent
validate_that(q2(P = c(2L,6L,5L)) == 7L)
# If we are facing (6,2,3,5) then
# we can pick 6 + 5 since they are not adjacent to each other
# and give the highest profit
validate_that(q2(P = c(6L,2L,3L,5L)) == 11L)
q3(B = 11L, C = c(2L, 1L, 4L, 3L, 1L),P = c(9L, 4L, 17L, 8L, 3L))
q3 <- memoise(function(B, C, P) {
if (!is.integer(B) || !is.integer(C) || !is.integer(P))
stop ("B, C and P must contain integers")
if (length(C) != length(P))
stop ("Both P and C must be the same length")
#N <- length(P) # the total number of gas station locations
# N should *NOT* be hard coded. The bot will try to feed P which has 10 locations or more.
# Write your code here
D <- C <= B
C <- C[D]
P <- P[D]
N <- length(P)
if (B < min(C) | N <= 0L | B <= 0L) return(0L)  #problem is NA's are in the list of P here
if (N == 1L) return(P[1])
if (N == 2L) return(max(P))
max(
( ifelse(B - C[1]>=0, (q3(B - C[1], C[3:N], P[3:N]) + P[1]), 0)),
( q3(B, C[2:N], P[2:N]))
)
})
# Please print the result of that line below
q3(B = 11L, C = c(2L, 1L, 4L, 3L, 1L),P = c(9L, 4L, 17L, 8L, 3L))
if(!require(installr)) {
install.packages("installr");
require(installr)
}
updateR()
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyr)
library(caret)
library(lubridate)
library(Hmisc)
library(knitr)
library(tm)
library(topicmodels)
library(SnowballC)
set.seed(72)#setting seed right away
tweets <- read.csv('tweets.csv',header = TRUE)
users <- read.csv('users.csv', header = TRUE)
kable(head(tweets,5))
kable(head(users,5))
describe(tweets)
summary(tweets)
notime <- tweets %>% filter(is.na(tweets$created_at))
unique(as.character(notime$user_key))
notime$text
remove(notime) #keeping it clean
tweets <- tweets %>% filter(!is.na(tweets$created_at))
tweets$favorite_count[is.na(tweets$favorite_count)] <- 0
tweets$retweet_count[is.na(tweets$retweet_count)] <- 0
tweets$created_str <- as.Date(tweets$created_str,'%Y-%m-%d %H:%M:%S')
tweets$month_year <- floor_date(tweets$created_str, "month")
tweets$year <- year(tweets$created_str)
tweets_year <- tweets %>% group_by(year) %>% tally()
ggplot(tweets_year) + geom_bar(stat='identity', aes(year,n, fill = n))
remove(tweets_year)
tweet_month <- tweets %>% group_by(month_year) %>% tally
ggplot(tweet_month) + geom_line(aes(month_year,n))
#Let's examine the most popular tweets
top_20_favorite <- arrange(tweets, favorite_count) %>% tail(20) %>%
select(user_key,created_str,month_year,retweet_count,favorite_count,text,hashtags,mentions)
top_20_favorite
top_user <- left_join(tweets,users,by=c('user_key'='name'))
top_user <- top_user %>% group_by(user_key) %>%
summarise(total_fav = sum(favorite_count),
total_rt = sum(retweet_count),
total_tweets = n()
)
ggplot(top_user) + geom_density(aes(total_tweets)) + xlab('Total Tweets') + ylab('Number of users')
ggplot(top_user) + geom_density(aes(total_rt)) + xlab('Total Retweets') + ylab('Number of users')
ggplot(top_user) + geom_density(aes(total_fav)) + xlab('Total Favorites') + ylab('Number of users')
top_user$total_act <- top_user$total_fav + top_user$total_rt
top_user$act_ratio <- top_user$total_act / top_user$total_tweets
top_user <- arrange(top_user, act_ratio)
tail(top_user,5)
length(top_user$user_key[top_user$act_ratio == 0])
sum(top_user$total_tweets[top_user$act_ratio == 0])
#creating a corpus of tweets
tweet_doc <- Corpus(VectorSource(as.character(tweets$text)))
#removing punctuation
tweet_doc <- tm_map(tweet_doc, removePunctuation)
#removing numbers too
tweet_doc <- tm_map(tweet_doc, removeNumbers)
#putting it all to lower case
tweet_doc <- tm_map(tweet_doc, content_transformer(tolower))
#removing stop words with no specific topic
tweet_doc <- tm_map(tweet_doc, removeWords, stopwords("english"))
#remove any irrelevant white space
tweet_doc <- tm_map(tweet_doc, stripWhitespace)
#get down to just the stem words that are important
tweet_doc <- tm_map(tweet_doc, stemDocument)
tfm <- DocumentTermMatrix(tweet_doc)
#removing sparse rows
ui = unique(tfm$i)
tfm = tfm[ui,]
#removing sparse items in the matrix
tfm<-removeSparseTerms(tfm, sparse=0.99999) #since these are stemmed tweets i'm expecting to have a lot of sparseness
#setting the bar low for non-sparise items
print(dim(tfm))
#removing sparse items in the matrix
tfm<-removeSparseTerms(tfm, sparse=0.9999) #since these are stemmed tweets i'm expecting to have a lot of sparseness
print(dim(tfm))
set.seed(72)
results <- LDA(tfm, k = 20, method = "Gibbs")
#removing sparse rows
ui = unique(tfm$i)
tfm = tfm[ui,]
#removing sparse items in the matrix
tfm<-removeSparseTerms(tfm, sparse=0.9999) #since these are stemmed tweets i'm expecting to have a lot of sparseness
#removing sparse rows
ui = unique(tfm$i)
tfm = tfm[ui,]
print(dim(tfm))
set.seed(72)
results <- LDA(tfm, k = 20, method = "Gibbs")
#need to drop tweets with on content.
w= 5
thresh = 0.015 #was .015
Terms <- terms(results, w,thresh)
Terms
thresh = 0.05 #was .015
Terms <- terms(results, w,thresh)
Terms
w= 10
thresh = 0.05 #was .015
Terms <- terms(results, w,thresh)
Terms
results <- LDA(tfm, k = 10, method = "Gibbs")
w= 10
thresh = 0.05 #was .015
Terms <- terms(results, w,thresh)
Terms
thresh = 0.02 #was .015
Terms <- terms(results, w,thresh)
Terms
thresh = 0.01 #was .015
w= 10
thresh = 0.01 #was .015
Terms <- terms(results, w,thresh)
Terms
w= 10
thresh = 0.015 #was .015
Terms <- terms(results, w,thresh)
Terms
Terms10 <- terms(results, w,thresh)
Terms10
results <- LDA(tfm, k = 8, method = "Gibbs")
w= 10
thresh = 0.015 #was .015
Terms8 <- terms(results, w,thresh)
Terms8
w= 5
thresh = 0.015 #was .015
Terms8 <- terms(results, w,thresh)
Terms8
#creating a corpus of tweets
tweet_doc <- Corpus(VectorSource(as.character(tweets$text)))
#removing punctuation
tweet_doc <- tm_map(tweet_doc, removePunctuation)
#removing numbers too
tweet_doc <- tm_map(tweet_doc, removeNumbers)
#putting it all to lower case
tweet_doc <- tm_map(tweet_doc, content_transformer(tolower))
#removing stop words with no specific topic
tweet_doc <- tm_map(tweet_doc, removeWords, stopwords("english"))
#remove any irrelevant white space
tweet_doc <- tm_map(tweet_doc, stripWhitespace)
#get down to just the stem words that are important
#tweet_doc <- tm_map(tweet_doc, stemDocument)
tfm <- DocumentTermMatrix(tweet_doc)
#removing sparse items in the matrix
tfm<-removeSparseTerms(tfm, sparse=0.9999) #since these are stemmed tweets i'm expecting to have a lot of sparseness
#setting the bar low for non-sparise items
#removing sparse rows
ui = unique(tfm$i)
tfm = tfm[ui,]
print(dim(tfm))
set.seed(72)
results <- LDA(tfm, k = 8, method = "Gibbs")
results <- LDA(tfm, k = 8, method = "Gibbs")
w= 5
w= 5
thresh = 0.015 #was .015
Terms10_nostem <- terms(results, w,thresh)
Terms10_nostem
w= 5
thresh = 0.01 #was .015
Terms10_nostem <- terms(results, w,thresh)
Terms10_nostem
Terms8_nostem <- terms(results, w,thresh)
Terms8_nostem
results <- LDA(tfm, k = 8, method = "Gibbs")
w= 5
thresh = 0.01 #was .015
Terms8_nostem <- terms(results, w,thresh)
Terms8_nostem
results <- LDA(tfm, k = 12, method = "Gibbs")
w= 5
thresh = 0.01 #was .015
Terms12_nostem <- terms(results, w,thresh)
Terms12_nostem
results <- LDA(tfm, k = 15, method = "Gibbs")
w= 5
thresh = 0.01 #was .015
Terms15_nostem <- terms(results, w,thresh)
Terms15_nostem
set.seed(72)
results <- LDA(tfm, k = 12, method = "Gibbs")
Terms15_nostem <- terms(results, w,thresh)
Terms12_nostem #this one is actually pretty solid
Terms12_nostem
Terms12_nostem[0]
Terms12_nostem[1]
topics <- []
topics <- list()
topics <- data.frame(Terms12_nostem)
Terms12_nostem[1]
Terms12_nostem[2]
Terms12_nostem[3]
Terms12_nostem[4]
Terms12_nostem[5]
Terms12_nostem[6]
Terms12_nostem[7]
Terms12_nostem[8]
Terms12_nostem[9]
Terms12_nostem[12]
Terms12_nostem[11]
Terms12_nostem[10]
setwd("~/GitHub/rtweet")
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyr)
library(caret)
library(lubridate)
library(Hmisc)
library(knitr)
library(tm)
library(topicmodels)
library(SnowballC)
set.seed(72)#setting seed right away
tweets <- read.csv('tweets.csv',header = TRUE)
users <- read.csv('users.csv', header = TRUE)
kable(head(tweets,5))
kable(head(users,5))
describe(tweets)
summary(tweets)
notime <- tweets %>% filter(is.na(tweets$created_at))
unique(as.character(notime$user_key))
notime$text
remove(notime) #keeping it clean
tweets <- tweets %>% filter(!is.na(tweets$created_at))
tweets$favorite_count[is.na(tweets$favorite_count)] <- 0
tweets$retweet_count[is.na(tweets$retweet_count)] <- 0
tweets$created_str <- as.Date(tweets$created_str,'%Y-%m-%d %H:%M:%S')
tweets$month_year <- floor_date(tweets$created_str, "month")
tweets$year <- year(tweets$created_str)
tweets_year <- tweets %>% group_by(year) %>% tally()
ggplot(tweets_year) + geom_bar(stat='identity', aes(year,n, fill = n))
remove(tweets_year)
tweet_month <- tweets %>% group_by(month_year) %>% tally
ggplot(tweet_month) + geom_line(aes(month_year,n))
#Let's examine the most popular tweets
top_20_favorite <- arrange(tweets, favorite_count) %>% tail(20) %>%
select(user_key,created_str,month_year,retweet_count,favorite_count,text,hashtags,mentions)
top_20_favorite
top_user <- left_join(tweets,users,by=c('user_key'='name'))
top_user <- top_user %>% group_by(user_key) %>%
summarise(total_fav = sum(favorite_count),
total_rt = sum(retweet_count),
total_tweets = n()
)
ggplot(top_user) + geom_density(aes(total_tweets)) + xlab('Total Tweets') + ylab('Number of users')
ggplot(top_user) + geom_density(aes(total_rt)) + xlab('Total Retweets') + ylab('Number of users')
ggplot(top_user) + geom_density(aes(total_fav)) + xlab('Total Favorites') + ylab('Number of users')
top_user$total_act <- top_user$total_fav + top_user$total_rt
top_user$act_ratio <- top_user$total_act / top_user$total_tweets
top_user <- arrange(top_user, act_ratio)
tail(top_user,5)
length(top_user$user_key[top_user$act_ratio == 0])
sum(top_user$total_tweets[top_user$act_ratio == 0])
#creating a corpus of tweets
tweet_doc <- Corpus(VectorSource(as.character(tweets$text)))
#removing punctuation
tweet_doc <- tm_map(tweet_doc, removePunctuation)
#removing numbers too
tweet_doc <- tm_map(tweet_doc, removeNumbers)
#putting it all to lower case
tweet_doc <- tm_map(tweet_doc, content_transformer(tolower))
#removing stop words with no specific topic
tweet_doc <- tm_map(tweet_doc, removeWords, stopwords("english"))
#remove any irrelevant white space
tweet_doc <- tm_map(tweet_doc, stripWhitespace)
tfm <- DocumentTermMatrix(tweet_doc)
#removing sparse items in the matrix
tfm<-removeSparseTerms(tfm, sparse=0.9999) #since these are stemmed tweets i'm expecting to have a lot of sparseness
#setting the bar low for non-sparise items
#removing sparse rows
ui = unique(tfm$i)
tfm = tfm[ui,]
print(dim(tfm))
set.seed(72)
results <- LDA(tfm, k = 12, method = "Gibbs")
w= 5
thresh = 0.01 #was .015
Terms12_nostem <- terms(results, w,thresh)
Terms12_nostem #this one is actually pretty solid
results <- LDA(tfm, k = 12, method = "Gibbs")
Terms12_nostem #this one is actually pretty solid
w= 8
thresh = 0.01 #was .015
Terms12_nostem <- terms(results, w,thresh)
Terms12_nostem #this one is actually pretty solid
