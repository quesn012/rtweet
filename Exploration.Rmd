---
title: "Exploration - RT"
author: "Bryce Quesnel"
date: "February 24, 2018"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyr)
library(caret)
library(lubridate)
library(Hmisc)
library(knitr)
library(tm)
library(topicmodels)
library(SnowballC) 

```


###Loading the data
```{r}
set.seed(72)#setting seed right away

tweets <- read.csv('tweets.csv',header = TRUE)
users <- read.csv('users.csv', header = TRUE)

```

#NOTE
This is currently a work in progress. The goal is focused on exploration and I try to throw time into it and try new things as I'm available to with grad school. All below this is subject to change.

Also note: this is purely exploratory. None of this is meant to be taken politically, the goal is just to explore a fun data set and try to find some cool stuff.

###Clean up what needs to be cleaned
```{r}
head(tweets,5)

head(users,5)
```


```{r, results = 'hide'}
#results are hidden in RMD to shorten document length

describe(tweets)

summary(tweets)
```

Notice we do have missing data around userID, createdAt, favoriteCount, tweetId, retweeted_status_ID and in_reply_to_status_id. I'm going to assume if there's missing for the reply/favorite items, then there was not favorite or it was not a reply and its a valid NA. As for the userID data... maybe the users were deleted since the tweets were created? Or maybe they just aren't included in this data set for whatever reason (Private, etc). I'm not a big twitter user or expert so it may be something relevant to look into. 

Additionally, the createdAt data is needed since we're focusing on doing a time analysis. I'm going to take a quick look at the ones missing time.

```{r}
notime <- tweets %>% filter(is.na(tweets$created_at))

unique(as.character(notime$user_key))

notime$text

```

We can see its 9 different users but oddly enough, no text of the tweets exist. I'm guessing these have since been deleted. I checked a few of the accounts and they've been suspended (no suprise) so we are going to filter these ones out for sake of keeping our analysis clean.

```{r}
remove(notime) #keeping it clean
tweets <- tweets %>% filter(!is.na(tweets$created_at))
```

A few other house keeping cleaning items...

```{r}
tweets$favorite_count[is.na(tweets$favorite_count)] <- 0

tweets$retweet_count[is.na(tweets$retweet_count)] <- 0
```

We also need to clean the dates and times.
```{r}
tweets$created_str <- as.Date(tweets$created_str,'%Y-%m-%d %H:%M:%S')

tweets$month_year <- floor_date(tweets$created_str, "month")
tweets$year <- year(tweets$created_str)
```


Let's look at the volume of tweets over time
```{r}
tweets_year <- tweets %>% group_by(year) %>% tally()
ggplot(tweets_year) + geom_bar(stat='identity', aes(year,n, fill = n))
```

```{r}
remove(tweets_year)
tweet_month <- tweets %>% group_by(month_year) %>% tally

ggplot(tweet_month) + geom_line(aes(month_year,n))

```

We can see a ton of activity occuring right around election time which isn't supriring, but also carrying forward the months after election into 2017 then rapidly dropping off.




Next thing we should do is look at some of the top and bottom tweets to try to get a little more domain knowledge on what's happening.

```{r}
#Let's examine the most popular tweets
top_20_favorite <- arrange(tweets, favorite_count) %>% tail(20) %>%
  select(user_key,created_str,month_year,retweet_count,favorite_count,text,hashtags,mentions)
top_20_favorite
```

Interesting! Off the bat we have all over the board stuff; a lot of Hillary related items, quite a few BLM tweets and some trump tweets. Interesting thing here is you see a lot of users have multiple tweets in the top 20; ten_gop alone has 6 of the top 20 favorited tweets. 

Speaking of this, lets look at users who have the most likes/retweets. this will be the join where we do summarises to get the sum of favorites/reetweets/etc

```{r}
top_user <- left_join(tweets,users,by=c('user_key'='name'))

top_user <- top_user %>% group_by(user_key) %>% 
  summarise(total_fav = sum(favorite_count),
            total_rt = sum(retweet_count),
            total_tweets = n()
            )


ggplot(top_user) + geom_density(aes(total_tweets)) + xlab('Total Tweets') + ylab('Number of users')
ggplot(top_user) + geom_density(aes(total_rt)) + xlab('Total Retweets') + ylab('Number of users')
ggplot(top_user) + geom_density(aes(total_fav)) + xlab('Total Favorites') + ylab('Number of users')
```

The above density plots are pretty ugly but it shows us something imprortant; most of the users were relatiely unknown and there were outliers to ones which were very successful. We'll see users with 1000s of tweets and not a single re-tweet or favorite; this mimics the actual usage of twitter where there is tons of traffic and a few actual very popular tweets.

To iterate this let's do a really simple calcluation to find the most popular accounts by activity...

```{r}
top_user$total_act <- top_user$total_fav + top_user$total_rt
top_user$act_ratio <- top_user$total_act / top_user$total_tweets
top_user <- arrange(top_user, act_ratio)
tail(top_user,5)
```

In the above we count an 'activity' as a retweet or a favorite. We take the total number of 'activity' divided by the total tweets to get the activity per tweet ratio. We can see a few of those same accounts we saw in our top most favorited tweets are here with the highest actvity per tweet ratio. 

```{r}
length(top_user$user_key[top_user$act_ratio == 0])
sum(top_user$total_tweets[top_user$act_ratio == 0])
```

Additionally, we can see there were 279 users who had NO activity whatsoever, and still created over 61,645 tweets! That's a little shy of a third of all the tweets coming from users who never had ANY actvity whatsoever. Still though, that means more than 2/3s the tweet came from accounts that were active with other twitter accounts in some way or another.


#Topic Modeling

So let's get to the core of this analysis: Topic modeling! I thought it would be interesting to run latent dirichlet allocation on this data set, since it would be interesting to understand what topics were being discussed in what frequency relative to what was occuring during that timeline. The results may not be too suprising but it would help us understand the topics that were emphasized to influence the election relative to what was going on in the greater political atmosphere.

Note: due to the nature we already discussed, the topics focused on may not represent the actual influence. It is very apparent based of retweets and favorites the quality of the tweet matters more than the quantity. That being said, let's see if topic modeling results make any sense.

```{r}
#creating a corpus of tweets
tweet_doc <- Corpus(VectorSource(as.character(tweets$text)))

#removing punctuation
tweet_doc <- tm_map(tweet_doc, removePunctuation)
#removing numbers too
tweet_doc <- tm_map(tweet_doc, removeNumbers)
#putting it all to lower case
tweet_doc <- tm_map(tweet_doc, content_transformer(tolower))
#removing stop words with no specific topic
tweet_doc <- tm_map(tweet_doc, removeWords, stopwords("english"))
#remove any irrelevant white space
tweet_doc <- tm_map(tweet_doc, stripWhitespace)
```

The above is just preparing to run topic models. We are stripping out all the things we don't care about; punctuation, numbers, stop words, white space etc. Note I did not stem the words in this case as I wanted to see the full outputs of different words due to the nature of things like twitter handles (real vs realDonaldTrump is an important distinction, etc). 

```{r, cache=TRUE}
set.seed(72)
tfm <- DocumentTermMatrix(tweet_doc)


#removing sparse items in the matrix
tfm<-removeSparseTerms(tfm, sparse=0.999) #since these are stemmed tweets i'm expecting to have a lot of sparseness
#setting the bar low for non-sparise items

#removing sparse rows
ui = unique(tfm$i)
tfm = tfm[ui,]


print(dim(tfm))
```

Above is our term frequency matrix. We use this term frequency matrix to run LDA on to find our topics. Note that I removed all rows without terms in the term frequecy matrix. Additionally, I removed the really scarce terms to hopefully decrease the size of the matrix and increase the speed at which this runs. Removing scarce terms means we can't generalize to all tweets (about 9% will drop out) but given twitter is not traditional and pure format for words since it can include pictures/images and a lot of non-traditional text (emoji's etc) I think this was an effective way to focus on the tweets that were most legible and relevant in regards to their text context.



```{r, cache=TRUE}
set.seed(72)
results <- LDA(tfm, k = 12, method = "Gibbs")


w= 8
thresh = 0.01 #was .015
Terms12_nostem <- terms(results, w,thresh) 


Terms12_nostem #this one is actually pretty solid
```

Aftering trying different thresholds and sizes, 12 seems to give us the clearest set of topics. Not every topic is perfect but I think we have a few ones that really stick out!


"Get/midnight/said"
```{r}
Terms12_nostem[1:8]
top = c()
top[1] = 'Get/Midnight/Said'
```

This was the most generic of the topics discovered. There was not much value in it, but there is consistency around things like get/got, said, etc. The one interesting point is the word midnight. As much as I want to try to summarise each topic, for this one I'm just going to list a few of the words as a title as at this point I'm having a hard time deriving any insighst from it.

"Liberal Media relative to conservatives/Trump"
```{r}
Terms12_nostem[9:16] 
top[2] = 'Liberal Media relative to conservatives/Trump'
```

This topic specific references a few liberal media sources (cnn/washington post) but also along side a popular conservative twitter handle (@conservatexian) as well as Trumps (which would also be Trump's in this case due to how we cleaned our words). My guess is this is a discussion of liberal media sources from other perspectives (most likely conservative ones that support Trump).

"Make America Great Again"
```{r}
Terms12_nostem[17:24] 
top[3] = 'MAGA'
```

This topic is relatively simple; focused on trump slogans like #MAGA and including Trump's actual twitter handle as well as PJnet which is the 'patriotic journalist network' set up for voicing conservative opinions on twitter. This a pro-Trump hash tag topic.


"General Positive Words"
```{r}
Terms12_nostem[25:32] 
top[4] = 'General Positive Words'
```

This topic is generally focused on positive words (love, good, right, life).


"General Action Words"
```{r}
Terms12_nostem[33:40] 
top[5] = 'General Action Words'
```

This topic is generally focused on action words (say, see, like, want, think).

"Obama & Time"
```{r}
Terms12_nostem[41:48] 
top[6] = 'Obama & Time'
```

This topic is focused on Obama (president, Obama) and also has a large constraint of time related words (today, day, year, time).

"American people & the election"
```{r}
Terms12_nostem[49:56] 
top[7] = 'American people & the election'
```

This topic is focused on the American people, as well as the election. Show/support/election are definetly action words used by a side to promote their candidate. That being said the interesting one here is never and media. My initial thoughts are its referring to the media negatively, and saying the American people will decide the election but that may be over playing my own thoughts so I'll keep the topic at American people & the election.

"Imperative call for action"
```{r}
Terms12_nostem[57:64] 
top[8] = 'Imperative call for action'
```

Another really interseting one, this topic uses very strong words (stop, die, must) that when combined with please make me think its imperative tweets, often request for action. With the combination of die in there, this may be a topic strongly related to either police brutality or overseas military conflict based off my initial readings of the tweets so far.

"Race & Gender"
```{r}
Terms12_nostem[65:72] 
top[9] = 'Race & Gender'
```

This topic is definetly related to race and gender in America. In addition to race (black/white) we also see police and @blicqer (popular black news twitter handle).


"Global topics and nationalism"
```{r}
Terms12_nostem[73:80] 
top[10] = 'Global topics and nationalis'
```

This is a global topic, definetly focused on American foreign relations and nationalism. Muslim and world are definetly global words. Work, take back and real all sound vaguely nationalistic as well.

"Trump communication to supporters"
```{r}
Terms12_nostem[81:88] 
top[11] = 'Trump communication to supporters'
```

This topic encompasses Trump, then a lot of words related to communication (live, watch, video, says) and his supporters. My guess is these tweets are about Trump addressing supporters, at rallies or via another form of communication.

"Clinton's campaign and FBI"
```{r}
Terms12_nostem[89:96] 
top[12] = "Clinton's campaign and FBI"
```

This is massively interesting to me. Hillary or any form of Hillary Clinton where only found in a single topic while Trump was seperate from his twitter handle (Hillary's handle was very absent compared to Trump's). Additinoally, this is a very strong collection of words and was the easiest to interpret. 


Now that we have our topics lets do some further analysis! 

Let's look at volume by topic, but even more relevant I think would be looking at how the topics adjusted over time and what topics were being pushed during whats points in the election. Before that though, I'm going to add in the topics to the original tweet data set so we can maniuplate the data with the full set of features.

```{r}
#find the tweets we used
index_tweets <- data.frame(id = as.numeric(tfm$dimnames$Docs), used = 'yes')

#create counting index in tweets
tweets_used <- tweets %>% mutate(id = seq.int(nrow(tweets)))

#merging index to tweets used. any NA tweets were not used in our analysis and will be filtered out
tweets_used <- left_join(tweets_used,index_tweets, by='id')
tweets_used <- tweets_used %>% filter(used == 'yes')
```

First I narrowed down my list of tweets to just those that were used to build our term-frequency matrix (ie: those that stayed on topic). This swas 191k out of 208k so it was over 90% of the total tweets.

```{r}
tweets_used$tweet_top <- topics(results,1)

top_df <- data.frame(index = seq.int(12), topic = top)

tweets_used <- left_join(tweets_used,top_df,by=c('tweet_top'='index'))
```

The above gives us the topic for each document out of 191,101 elements. We then merge that in with each corresponding tweet and can see the primary topic of each tweet.

```{r}
ggplot(tweets_used) + geom_bar(aes(topic, fill = topic)) +  theme(axis.text.x=element_blank(),
                                                                  axis.ticks.x=element_blank(),
                                                                  axis.title.x=element_blank()) + ylab('Tweets')

```

The more generic topic (general*, get/midnight/said) have a pretty high volume overall which makes sense. But additionally, we see the most popular relevant topics are 'imperative call for action' and 'liberal media relative to conservatives/Trump'. Lower via volume are toipcs on 'race & gender' as well as 'trump communications to supporters'. That being said, we've already established that volume of tweets is a VERY poor way to establish impact. 60k+ tweets were from users without a single interaction in on any of their tweets. Let's instead look by favorite/retweets.

```{r}
fav_rt_topic <- tweets_used %>% group_by(topic) %>%
  summarise(
    favorites = sum(favorite_count),
    retweets = sum(retweet_count)
  )

ggplot(fav_rt_topic) + geom_bar(stat = 'identity',aes(topic,favorites, fill = topic)) +  theme(axis.text.x=element_blank(),
                                                                  axis.ticks.x=element_blank(),
                                                                  axis.title.x=element_blank()) + ylab('favorites')
```

```{r}
ggplot(fav_rt_topic) + geom_bar(stat = 'identity',aes(topic,retweets, fill = topic)) +  theme(axis.text.x=element_blank(),
                                                                  axis.ticks.x=element_blank(),
                                                                  axis.title.x=element_blank()) + ylab('favorites')

```

When looking by interactions, we can see that the general topics get very little interaction here but the tweets that get the most favorites and retweets are 'Clinton's campaign and FBI', 'American people & the election' and 'Race & Gender'. So the successful tweets were really focused on those specific topics as opposed to being generic. The Clinton/FBI tweets stand out as significantly more than any other topic.


The other important analysis to look at is what topics were being pushed over time and did the topics being discussed in these tweets adjust relative to current events.

```{r}
tweets_used$one <- 1

#ggplot(tweets_used) + geom_area(aes(month_year,favorite_count,fill=topic))

```




#Ideas to explore
should do a merge at some point to get user details on the tweet level. and then can look at location related items.
  could also look at timeline of how follower count changes over time
  
  look at unique users over time. how many did they add in over the life span, and the quantity of tweets over the lifespan?
  
finally do a clustering on users. make them into groups if that makes sense. would be good to showcase dimensionality reductin like pca
could use reg expressoin to find common thing like trump mentions, obama mentions, hilaary mention, etc. and then use those as features in clustering

could also do clustering by hastags...  see which were successful or started trending and which caught up?

Focus on good visuals!

